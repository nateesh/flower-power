{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2194,"status":"ok","timestamp":1665554116130,"user":{"displayName":"Nguyen Nina (Nina)","userId":"03910831052311598404"},"user_tz":-600},"id":"Rdgl52rlqiVB"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\natha\\anaconda3\\envs\\ml\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import pathlib\n","import os\n","import datetime\n","import time\n","\n","import numpy as np\n","# import tensorflow as tf\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras import layers, Model, utils, optimizers, losses\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1665554116131,"user":{"displayName":"Nguyen Nina (Nina)","userId":"03910831052311598404"},"user_tz":-600},"id":"XdBOo1OiqiVE"},"outputs":[],"source":["# model expected shape=(None, 224, 224)\n","IMG_SIZE = (224, 224)\n","IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n","flowers_dir = 'small_flower_dataset/'\n","EPOCHS = 20"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":615,"status":"ok","timestamp":1665554120938,"user":{"displayName":"Nguyen Nina (Nina)","userId":"03910831052311598404"},"user_tz":-600},"id":"91_Bs4E9qiVH","outputId":"f7004290-be50-4e8f-cdbe-397f534feb9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 886 files belonging to 5 classes.\n","Using 621 files for training.\n","Found 886 files belonging to 5 classes.\n","Using 265 files for validation.\n"]}],"source":["\n","batch_size = 32\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","                flowers_dir,\n","                labels='inferred',\n","                label_mode='int',\n","                class_names=None,\n","                color_mode='rgb',\n","                batch_size=batch_size,\n","                image_size=IMG_SIZE,\n","                shuffle=True,\n","                seed=2,\n","                validation_split=0.3,\n","                subset=\"training\",\n","                interpolation='bilinear',\n","                follow_links=False,\n","                crop_to_aspect_ratio=False)\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","                flowers_dir,\n","                labels='inferred',\n","                label_mode='int',\n","                class_names=None,\n","                color_mode='rgb',\n","                batch_size=batch_size,\n","                image_size=IMG_SIZE,\n","                shuffle=True,\n","                seed=2,\n","                validation_split=0.3,\n","                subset=\"validation\",\n","                interpolation='bilinear',\n","                follow_links=False,\n","                crop_to_aspect_ratio=False)\n","class_names = train_ds.class_names\n","\n","testing_ds = val_ds.take(2)\n","val_ds = val_ds.skip(2)\n","\n","    \n","\n","# Configure dataset for performance\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","testing_ds = testing_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","\n","# Standardize the data\n","# The RGB channel values are in the [0, 255] range. \n","# This is not ideal for a neural network; in general you should seek to make your input values small.\n","normalization_layer = layers.Rescaling(1./255)\n","normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n","normalized_testing_ds = testing_ds.map(lambda x, y: (normalization_layer(x), y))\n","\n","train_ds = normalized_train_ds\n","val_ds = normalized_val_ds\n","testing_ds = normalized_testing_ds"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=True, weights=\"imagenet\")\n","    \n","# Freeze layer exclude new layer\n","for layer in base_model.layers:\n","    layer.trainable=False\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["20/20 [==============================] - 8s 335ms/step\n","7/7 [==============================] - 3s 333ms/step\n","2/2 [==============================] - 1s 359ms/step\n"]}],"source":["feature_extractor = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n","# feature_extractor.summary()\n","\n","#feature extraction\n","train_ds_features = feature_extractor.predict(train_ds)\n","val_ds_features = feature_extractor.predict(val_ds)\n","test_ds_features = feature_extractor.predict(testing_ds)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["(64, 1280)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_ds_features.shape\n","val_ds_features.shape\n","test_ds_features.shape"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# extract the labels from the _ds batches and concatenate them into one array\n","train_labels = np.concatenate([y for x, y in train_ds], axis=0)\n","val_labels = np.concatenate([y for x, y in val_ds], axis=0)\n","test_labels = np.concatenate([y for x, y in testing_ds], axis=0)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," img (InputLayer)            [(None, 1280)]            0         \n","                                                                 \n"," dense (Dense)               (None, 256)               327936    \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 1285      \n","                                                                 \n","=================================================================\n","Total params: 329,221\n","Trainable params: 329,221\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","20/20 [==============================] - 1s 19ms/step - loss: 1.8755 - accuracy: 0.2158 - val_loss: 1.5065 - val_accuracy: 0.3632\n","Epoch 2/20\n","20/20 [==============================] - 0s 7ms/step - loss: 1.4640 - accuracy: 0.3591 - val_loss: 1.6768 - val_accuracy: 0.3134\n","Epoch 3/20\n","20/20 [==============================] - 0s 7ms/step - loss: 1.3774 - accuracy: 0.4283 - val_loss: 1.6883 - val_accuracy: 0.1940\n","Epoch 4/20\n","20/20 [==============================] - 0s 7ms/step - loss: 1.2157 - accuracy: 0.5266 - val_loss: 1.7368 - val_accuracy: 0.2488\n","Epoch 5/20\n","20/20 [==============================] - 0s 6ms/step - loss: 1.1378 - accuracy: 0.5733 - val_loss: 1.7471 - val_accuracy: 0.2637\n","Epoch 6/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.9781 - accuracy: 0.6441 - val_loss: 1.6441 - val_accuracy: 0.3035\n","Epoch 7/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.8004 - accuracy: 0.7391 - val_loss: 1.7503 - val_accuracy: 0.2338\n","Epoch 8/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.7101 - accuracy: 0.7955 - val_loss: 1.9615 - val_accuracy: 0.2239\n","Epoch 9/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.6327 - accuracy: 0.7858 - val_loss: 1.8395 - val_accuracy: 0.2239\n","Epoch 10/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4787 - accuracy: 0.8760 - val_loss: 2.0673 - val_accuracy: 0.2637\n","Epoch 11/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.4228 - accuracy: 0.8824 - val_loss: 2.4559 - val_accuracy: 0.1891\n","Epoch 12/20\n","20/20 [==============================] - 0s 7ms/step - loss: 0.3415 - accuracy: 0.9195 - val_loss: 2.4902 - val_accuracy: 0.1841\n","Epoch 13/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2778 - accuracy: 0.9452 - val_loss: 1.9423 - val_accuracy: 0.2736\n","Epoch 14/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.2198 - accuracy: 0.9614 - val_loss: 2.5668 - val_accuracy: 0.1940\n","Epoch 15/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1991 - accuracy: 0.9581 - val_loss: 2.1936 - val_accuracy: 0.2637\n","Epoch 16/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9871 - val_loss: 2.7088 - val_accuracy: 0.1642\n","Epoch 17/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1382 - accuracy: 0.9823 - val_loss: 2.4760 - val_accuracy: 0.2239\n","Epoch 18/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9823 - val_loss: 2.3696 - val_accuracy: 0.2438\n","Epoch 19/20\n","20/20 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 0.9871 - val_loss: 2.3794 - val_accuracy: 0.2935\n","Epoch 20/20\n","20/20 [==============================] - 0s 7ms/step - loss: 0.0941 - accuracy: 0.9936 - val_loss: 2.5515 - val_accuracy: 0.2587\n"]}],"source":["inputs = tf.keras.Input(shape=(1280), name=\"img\")\n","# x = layers.GlobalAveragePooling2D(inputs)\n","x = layers.Dense(256, activation='relu')(inputs)\n","x = layers.Dropout(0.2)(x)\n","outputs = layers.Dense(5, activation='softmax')(x)\n","\n","model = Model(inputs = inputs, outputs = outputs)\n","\n","model.summary()\n","\n","model.compile(\n","    optimizer=optimizers.SGD(learning_rate=0.1, momentum=0, nesterov=False),\n","    loss=losses.SparseCategoricalCrossentropy(),\n","    metrics=[\"accuracy\"])\n","    \n","history = model.fit(train_ds_features, train_labels, epochs=EPOCHS, validation_data=(val_ds_features, val_labels))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","\n","epochs_range = range(EPOCHS)\n","# plt.figure(figsize=(8, 8))\n","\n","\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 ('ml')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5f83ee68f34074522a5cb7607425e1d6147877ebdc6888c894829050fcfeee4d"}}},"nbformat":4,"nbformat_minor":0}
